{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word complexity 2.0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4tHzN_VQnX5",
        "outputId": "9c002129-2cf5-4f9d-e7d5-b4500efbaaa0"
      },
      "source": [
        "! rm -f dale_chall.py\n",
        "! wget https://raw.githubusercontent.com/artificial-intelligence-ml-cti/ml_cti/main/proiect/dale_chall.py"
      ],
      "execution_count": 1200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-21 20:02:37--  https://raw.githubusercontent.com/artificial-intelligence-ml-cti/ml_cti/main/proiect/dale_chall.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27456 (27K) [text/plain]\n",
            "Saving to: ‘dale_chall.py’\n",
            "\n",
            "\rdale_chall.py         0%[                    ]       0  --.-KB/s               \rdale_chall.py       100%[===================>]  26.81K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-11-21 20:02:37 (18.0 MB/s) - ‘dale_chall.py’ saved [27456/27456]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJA8kpUYQrmN",
        "outputId": "299fa9be-166a-4319-ec90-8c5d194ebdde"
      },
      "source": [
        "!pip install pyphen nltk pandas sklearn openpyxl"
      ],
      "execution_count": 1201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glhzENwSQt0y",
        "outputId": "96908f43-6ad2-4211-eb97-e670386cf89a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLS0xcqFQuDD",
        "outputId": "534be704-2e03-4f51-95a8-28934651d1ed"
      },
      "source": [
        "! rm -rf data*\n",
        "! wget https://github.com/artificial-intelligence-ml-cti/ml_cti/raw/main/proiect/data.zip\n",
        "! unzip \"data.zip\"\n",
        "\n",
        "! echo \"***\\n Fisierele sunt: \"\n",
        "! ls data/\n",
        "! echo \"****\\n Calea catre directorul cu date este: \"\n",
        "! readlink -f data/"
      ],
      "execution_count": 1203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-21 20:02:40--  https://github.com/artificial-intelligence-ml-cti/ml_cti/raw/main/proiect/data.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/artificial-intelligence-ml-cti/ml_cti/main/proiect/data.zip [following]\n",
            "--2021-11-21 20:02:40--  https://raw.githubusercontent.com/artificial-intelligence-ml-cti/ml_cti/main/proiect/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 741506 (724K) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>] 724.13K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-11-21 20:02:41 (12.0 MB/s) - ‘data.zip’ saved [741506/741506]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/test.xlsx          \n",
            "  inflating: data/train.xlsx         \n",
            "***\\n Fisierele sunt: \n",
            "test.xlsx  train.xlsx\n",
            "****\\n Calea catre directorul cu date este: \n",
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwSGJf1UQw0i"
      },
      "source": [
        "import pyphen\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# puteti incerca si KNN de la laboratorul 3\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# https://en.wikipedia.org/wiki/Precision_and_recall#Imbalanced_data\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "# https://readabilityformulas.com/new-dale-chall-readability-formula.php\n",
        "from dale_chall import DALE_CHALL"
      ],
      "execution_count": 1204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKk7WiJJQzsJ"
      },
      "source": [
        "dtypes = {\"sentence\": \"string\", \"token\": \"string\", \"complexity\": \"float64\"}\n",
        "train = pd.read_excel('/content/data/train.xlsx', dtype=dtypes, keep_default_na=False)\n",
        "test = pd.read_excel('/content/data/test.xlsx', dtype=dtypes, keep_default_na=False)"
      ],
      "execution_count": 1205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJniCMS9TjYB"
      },
      "source": [
        "# **Word structure features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w0nE6TfTIE1",
        "outputId": "7d49674f-023d-47c4-d85d-0ccbe2bc442b"
      },
      "source": [
        "def is_dale_chall(word):\n",
        "  '''Daca e in lista Dale-Chall sau nu.\n",
        "  Atentie daca cuvantul e scris cu litera mare sau mica.\n",
        "  '''\n",
        "  return int(word in DALE_CHALL)\n",
        "  \n",
        "  \n",
        "print(is_dale_chall('car'))\n",
        "print(is_dale_chall('Car'))\n",
        "print(is_dale_chall('supercalifragilistic'))"
      ],
      "execution_count": 1206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvFHcqayTIG_",
        "outputId": "ba581861-6841-4cfe-ee2e-283edfe630f5"
      },
      "source": [
        "def is_title(word):\n",
        "  '''Indica ca avem de a face cu o entitate.\n",
        "  Care de obicei au o complexitate conceptuala mai mare.\n",
        "  '''\n",
        "  return int(word.istitle())\n",
        "\n",
        "print(is_title('Hello World'))\n",
        "print(is_title('hoi'))"
      ],
      "execution_count": 1207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkBax7R9TII9"
      },
      "source": [
        "import pyphen\n",
        "dic = pyphen.Pyphen(lang='en')\n",
        "def nr_syllables(word):\n",
        "  return len(dic.inserted(word).split('-'))\n",
        "  "
      ],
      "execution_count": 1208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr7JXJPZTILD"
      },
      "source": [
        "vowels = 'aeiouw'\n",
        "def nr_vowels(word):\n",
        "    nr = 0\n",
        "    for chr in word:\n",
        "        if chr in vowels:\n",
        "            nr += 1\n",
        "    return nr"
      ],
      "execution_count": 1209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6E2MiHbTINJ"
      },
      "source": [
        "def get_word_structure_features(word):\n",
        "    '''Caracteristici la nivelul structurii unui cuvant.\n",
        "    '''\n",
        "    features = []\n",
        "    features.append(is_dale_chall(word))\n",
        "    features.append(is_title(word))\n",
        "    features.append(nr_syllables(word))\n",
        "    features.append(len(word))\n",
        "    features.append(nr_vowels(word))\n",
        "    return np.array(features)"
      ],
      "execution_count": 1210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2rp22O0TsaQ"
      },
      "source": [
        "# **Corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-ydyWuOTIPo"
      },
      "source": [
        "def corpus_feature(corpus):\n",
        "  '''E din corpusul bible, biomed sau europarl.\n",
        "  Atentie sunt categorical features.\n",
        "  '''\n",
        "  #d = {\"bible\": [1, 0, 0], \"europarl\": [0, 1, 0], \"biomed\": [0, 0, 1]}\n",
        "  d = {\"bible\": [0], \"europarl\": [1], \"biomed\": [2]}\n",
        "  return d[corpus]"
      ],
      "execution_count": 1211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II4fptSoTyBr"
      },
      "source": [
        "# **Wordnet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uepiwRsMTIRv"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "def nr_synsets(word):\n",
        "    return len(wn.synsets(word))\n",
        "  "
      ],
      "execution_count": 1212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09z8xbUANKy"
      },
      "source": [
        "def get_hyponyms(word):\n",
        "  hyper= wn.synsets(word)\n",
        "  sum=0\n",
        "  for i in hyper:\n",
        "    sum=sum+len(i.hypernyms())\n",
        "  return sum"
      ],
      "execution_count": 1213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG6VpZhWD4PD"
      },
      "source": [
        "def get_hypernyms(word):\n",
        "  hypo= wn.synsets(word)\n",
        "  sum=0\n",
        "  for i in hypo:\n",
        "    sum=sum+len(i.hyponyms())\n",
        "  return sum"
      ],
      "execution_count": 1214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEv7yzcLEhfq"
      },
      "source": [
        "def get_meronyms_parts(word):\n",
        "  hypo= wn.synsets(word)\n",
        "  sum=0\n",
        "  for i in hypo:\n",
        "    sum=sum+len(i.part_meronyms())\n",
        "  return sum"
      ],
      "execution_count": 1215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1tWR4JDFB03"
      },
      "source": [
        "def get_meronyms_substances(word):\n",
        "  hypo= wn.synsets(word)\n",
        "  sum=0\n",
        "  for i in hypo:\n",
        "    sum=sum+len(i.substance_meronyms())\n",
        "  return sum"
      ],
      "execution_count": 1216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-frbIDwFJCF"
      },
      "source": [
        "def get_holonyms_parts(word):\n",
        "  holo= wn.synsets(word)\n",
        "  sum=0\n",
        "  for i in holo:\n",
        "    sum=sum+len(i.part_holonyms())\n",
        "  return sum"
      ],
      "execution_count": 1217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igTgdrmaFW3S"
      },
      "source": [
        "def get_holonyms_substances(word):\n",
        "  holo= wn.synsets(word)\n",
        "  sum=0\n",
        "  for i in holo:\n",
        "    sum=sum+len(i.substance_holonyms())\n",
        "  return sum"
      ],
      "execution_count": 1218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GSiHEvGTIUj"
      },
      "source": [
        "def get_wordnet_features(word):\n",
        "  features = []\n",
        "  features.append(nr_synsets(word))\n",
        "  features.append(get_hyponyms(word))\n",
        "  features.append(get_hypernyms(word))\n",
        "  features.append(get_meronyms_parts(word))\n",
        "  features.append(get_meronyms_substances(word))\n",
        "  features.append(get_holonyms_parts(word))\n",
        "  features.append(get_holonyms_substances(word))\n",
        "  return np.array(features)"
      ],
      "execution_count": 1219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jdkywBg70hg"
      },
      "source": [
        "# test = wn.synsets('bird')\n",
        "# print(test)\n",
        "# sum=0\n",
        "# for i in test:\n",
        "#   sum=sum+len(i.hyponyms())\n",
        "\n",
        "# print(sum)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X05PLdf3U4A6"
      },
      "source": [
        "# **Context**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmpFLuQAU85m",
        "outputId": "fcd6e28c-1977-4b37-9014-68b1b2ac5fd9"
      },
      "source": [
        "def nearby_words_complexity(sentence):\n",
        "  word = sentence.split(' ')\n",
        "  features = []\n",
        "  for i in word:\n",
        "    features.append(get_word_structure_features(i)[0])\n",
        "  features= np.mean(features)\n",
        "  return features\n",
        "\n",
        "print(nearby_words_complexity(\"Azi merg la mare\"))\n"
      ],
      "execution_count": 1221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUJQslOAU-yu"
      },
      "source": [
        "def get_context_features(sentence):\n",
        "  features = []\n",
        "  features.append(nearby_words_complexity(sentence))\n",
        "  return np.array(features)\n",
        "\n"
      ],
      "execution_count": 1222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAMCNP95UFwY"
      },
      "source": [
        "# **Append all Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGgzBSmcTIW0"
      },
      "source": [
        "def featurize(row):\n",
        "    word = row['token']\n",
        "    all_features = []\n",
        "    all_features.extend(corpus_feature(row['corpus']))\n",
        "    all_features.extend(get_word_structure_features(word))\n",
        "    all_features.extend(get_wordnet_features(word))\n",
        "    all_features.extend(get_context_features(row['sentence']))\n",
        "    return np.array(all_features)"
      ],
      "execution_count": 1223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCIYVUshTIY8"
      },
      "source": [
        "def featurize_df(df):\n",
        "    nr_of_features = len(featurize(df.iloc[0]))\n",
        "    print('nr de features este ', nr_of_features)\n",
        "    nr_of_examples = len(df)\n",
        "    print('nr de exemple este ', nr_of_examples)\n",
        "    features = np.zeros((nr_of_examples, nr_of_features))\n",
        "    for index, row in df.iterrows():\n",
        "        row_ftrs = featurize(row)\n",
        "        features[index, :] = row_ftrs\n",
        "    return features"
      ],
      "execution_count": 1224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNsJ0C31TIbb",
        "outputId": "261a2eb7-340c-48db-e61a-44fe953c9401"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "X_train = preprocessing.normalize(featurize_df(train))\n",
        "print('Datele de antrenare au forma ', X_train.shape)\n",
        "y_train = train['complex'].values\n",
        "print('Etichetele datelor de antrenare arata cam asa: ', y_train)"
      ],
      "execution_count": 1225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nr de features este  14\n",
            "nr de exemple este  7662\n",
            "Datele de antrenare au forma  (7662, 14)\n",
            "Etichetele datelor de antrenare arata cam asa:  [0 0 0 ... 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-_Tx3AvdcAT"
      },
      "source": [
        "# **TESTARE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf5RynKSdkdJ"
      },
      "source": [
        ""
      ],
      "execution_count": 1225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyP2jPtZdPfV",
        "outputId": "f3224d68-1ab6-47db-bf2c-42853416ca2c"
      },
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_traincopy=X_train\n",
        "y_traincopy= y_train\n",
        "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=7)\n",
        "model.fit(X_train2, y_train2)\n",
        "\n",
        "preds = model.predict(X_valid)\n",
        "\n",
        "print(balanced_accuracy_score(y_valid, preds))"
      ],
      "execution_count": 1226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6730529230529231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaP9DLvXdi2b"
      },
      "source": [
        "# **APLICARE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YDXLtEjTIds",
        "outputId": "7de4b3d8-e4c6-47a7-e81a-273fb90a8fc7"
      },
      "source": [
        "X_test = preprocessing.normalize(featurize_df(test))\n",
        "print(X_test)"
      ],
      "execution_count": 1227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nr de features este  14\n",
            "nr de exemple este  1338\n",
            "[[0.12803688 0.         0.12803688 ... 0.         0.         0.        ]\n",
            " [0.07944127 0.         0.         ... 0.         0.         0.05362286]\n",
            " [0.07093031 0.         0.         ... 0.03546516 0.         0.00818427]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.10520384]\n",
            " [0.12028661 0.         0.         ... 0.         0.         0.04059673]\n",
            " [0.         0.         0.         ... 0.         0.         0.05058793]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boDD3Y39R3js"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "# print(X_train2.shape)\n",
        "# print(X_valid.shape)\n",
        "# print(np.bincount(y_train2))\n",
        "# print(np.bincount(y_valid))"
      ],
      "execution_count": 1228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCsjO-dxa-NY"
      },
      "source": [
        "# from sklearn import svm\n",
        "# from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
        "\n",
        "# for C in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "#     print('pentru C = ', C)\n",
        "#     model = svm.LinearSVC(C=C)\n",
        "#     model.fit(X_train2, y_train2)\n",
        "#     # e la fel de bun ca si cum as prezice 0 de fiecare data:\n",
        "#     preds = model.predict(X_valid)\n",
        "#     #preds = np.zeros(len(y_valid))\n",
        "#     print(balanced_accuracy_score(y_valid, preds))\n",
        "#     print(accuracy_score(y_valid, preds))\n",
        "  \n"
      ],
      "execution_count": 1229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zwl2A3ma-P_"
      },
      "source": [
        "\n",
        "\n",
        "# model = KNeighborsClassifier(n_neighbors=7)\n",
        "# model.fit(X_traincopy, y_traincopy)\n",
        "\n",
        "# preds = model.predict(X_test)\n",
        "# print(sum(preds))"
      ],
      "execution_count": 1230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbbCMlKWa-Sd",
        "outputId": "ae1a5416-79b7-475f-e6b2-76a60937c2a3"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "#model = svm.LinearSVC(C=1)\n",
        "model = Perceptron()\n",
        "\n",
        "#model = KNeighborsClassifier(n_neighbors=7)\n",
        "model.fit(X_traincopy, y_traincopy)\n",
        "preds = model.predict(X_test)\n",
        "#print('accuratete pe train ', balanced_accuracy_score(y_train2, preds))\n",
        "# print('acuratete pe valid ', balanced_accuracy_score(y_valid, preds))\n",
        "print(np.sum(preds))\n"
      ],
      "execution_count": 1231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BsLck-qa-VG"
      },
      "source": [
        ""
      ],
      "execution_count": 1231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KMnww2sa-Xq"
      },
      "source": [
        ""
      ],
      "execution_count": 1231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDNd7UBYa-Z5"
      },
      "source": [
        ""
      ],
      "execution_count": 1231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzSHCUWJawgV"
      },
      "source": [
        "# **Creare csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-RiuyJ4a1Kp"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['id'] = test.index + len(train) + 1\n",
        "df['complex'] = preds\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 1232,
      "outputs": []
    }
  ]
}